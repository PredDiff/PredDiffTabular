{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# (Tabular) MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import functools\n",
    "\n",
    "import pred_diff.preddiff as preddiff\n",
    "from pred_diff.imputers import impute\n",
    "from pred_diff.imputers import vae_impute as vae\n",
    "from pred_diff.tools import utils_mnist as ut_mnist\n",
    "from pred_diff.tools import init_plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# paper style\n",
    "init_plt.update_rcParams(fig_width_pt=234.88*2)\n",
    "\n",
    "# default\n",
    "# plt.style.use('default')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load MNIST data and train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model, df_train, df_test, target_test = ut_mnist.get_model_and_data(max_epochs=1, retrain=False)\n",
    "print(f\"How much overconfident is the model?\\n\"\n",
    "      f\"temperature rescaling factor: model.T = {model.T:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Select parameter for *PredDiff*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = model.return_accurary(data=df_test, target=target_test)\n",
    "print(f'model accuracy on testset: {acc*100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "imputer_selection = 'TrainSet'\n",
    "# imputer_selection = 'TrainSetMahalonobis'\n",
    "# imputer_selection = 'VAE'\n",
    "\n",
    "n_group = 10\n",
    "\n",
    "# possible values: 1, 2, 4, 7, 14\n",
    "filter_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "iparam = ut_mnist.ImgParams(n_pixel=28, block_size=filter_size)\n",
    "\n",
    "if imputer_selection == 'TrainSet':\n",
    "    explainer = preddiff.PredDiff(model, df_train, imputer_cls=impute.TrainSetImputer, regression=False, n_jobs=8, \n",
    "                                  n_group=n_group, fast_evaluation=True)\n",
    "elif imputer_selection == 'TrainSetMahalonobi':\n",
    "    explainer = preddiff.PredDiff(model, df_train, imputer_cls=impute.TrainSetMahalanobisImputer, regression=False,\n",
    "                                  n_jobs=8, n_estimators=20, n_group=n_group, fast_evaluation=True)\n",
    "\n",
    "elif imputer_selection == 'VAE':\n",
    "    def df_round_clip(df):\n",
    "        return df.clip(lower=0, upper=1).div(1. / 255).round().div(255.)\n",
    "    explainer = preddiff.PredDiff(model, df_train, imputer_cls=vae.VAEImputer, regression=False, gpus=0, epochs=2,\n",
    "                           custom_postprocessing_fn=df_round_clip, n_group=n_group, fast_evaluation=True)\n",
    "\n",
    "else:\n",
    "    assert False, f'please enter a valid imputer_selection = {imputer_selection}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Select data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_selection = 'PaperSelection'\n",
    "# data_selection = 'RandomSelection'\n",
    "# data_selection = 'All'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if data_selection == 'PaperSelection':\n",
    "    data = df_test.iloc[[4, 15, 84, 9]]         # one digits each: 4, 5, 8, 9\n",
    "elif data_selection == 'RandomSelection':\n",
    "    data = df_test.iloc[np.random.randint(low=0, high=df_test.shape[0], size=2)]\n",
    "elif data_selection == 'All':\n",
    "    data = df_test.iloc[:]\n",
    "else:\n",
    "    assert False, f'please enter a valid data_selection = {data_selection}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# calculate relevances\n",
    "m_relevance, prediction_prob, m_list = ut_mnist.get_relevances(explainer=explainer, data=data, img_params=iparam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_selection = 'PredictedClass'\n",
    "# plot_selection = 'FourClasses'\n",
    "# plot_selection = 'AllClasses'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for img_id in np.arange(data.shape[0]):\n",
    "    n_importance = 1\n",
    "    i_reference = ut_mnist.get_reference_pixel(m_relevance=m_relevance, prediction_prob=prediction_prob,\n",
    "                                               img_id=img_id, n_importance=n_importance)\n",
    "\n",
    "    m_interaction = ut_mnist.get_interaction(explainer=explainer, data=data, iparam=iparam, m_list=m_list,\n",
    "                                             i_reference=i_reference)\n",
    "\n",
    "    i_vertical, i_horizontal = divmod(i_reference, iparam.max_index)\n",
    "\n",
    "    rect = functools.partial(ut_mnist.plot_rect, i_reference=i_reference, iparam=iparam)\n",
    "\n",
    "    if plot_selection == 'PredictedClass':\n",
    "        ut_mnist.plot_predicted_digit(relevance=m_relevance, interaction=m_interaction, prob_classes=prediction_prob,\n",
    "                                      data_digit=data, rect=rect, img_params=iparam, image_id=img_id)\n",
    "\n",
    "    elif plot_selection == 'FourClasses':\n",
    "        ut_mnist.plot_comparison(m_list_collected=m_relevance, prob_classes=prediction_prob, data_digit=data,\n",
    "                                 img_params=iparam, image_id=img_id)\n",
    "\n",
    "    elif plot_selection == 'AllClasses':\n",
    "        ut_mnist.plot_all_digits(m_list_collected=m_relevance, prob_classes=prediction_prob, data=data,\n",
    "                                 img_params=iparam, img_id=img_id, imputer='trainset')\n",
    "        ut_mnist.plot_all_digits(m_list_collected=m_interaction, prob_classes=prediction_prob, data=data,\n",
    "                                 img_params=iparam, img_id=img_id, imputer='interaction', rect=rect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (InterpretableML)",
   "language": "python",
   "name": "pycharm-7e489a09"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
